{"/":{"title":"Digital Garden Home ðŸŒ±","content":"\n[Map of contents](Map%20of%20contents.md)\n","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Activation-Energy":{"title":"Activation Energy","content":"Reference:  https://www.makeuseof.com/tag/7-mental-models-get-work-done/ | [Atomic Habits](Sources/Atomic%20Habits.md)    \nTopics: [Productivity](Topics/Productivity.md)  \n\n---\nThe more complex a task, the higher the activation energy required to start and sustain it.\n- To counter this, target the energy required to **start** a task.\n- Break down tasks into simpler and smaller steps.","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Authors/Fedor-Pikus":{"title":"Fedor Pikus","content":"","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Authors/James-Clear":{"title":"James Clear","content":"","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Authors/Leon-Hendrix":{"title":"Leon Hendrix","content":"","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Authors/Rhiannon-Beaubien":{"title":"Rhiannon Beaubien","content":"","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Authors/Scott-Meyers":{"title":"Scott Meyers","content":"","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Brain-Dump":{"title":"Brain Dump","content":"\nTopics: [Productivity](Topics/Productivity.md)  \nReference:[Leon Hendrix - Journaled for 1000 days. What I learned.](Sources/Leon%20Hendrix%20-%20Journaled%20for%201000%20days.%20What%20I%20learned..md) | [Getting Things Done](Getting%20Things%20Done.md)  \n\n---\n\nA technique used to offload all of the things that are currently straining you mentally by writing everything you can think of onto a note.\n\nValuable for reducing anxiety and staying productive as the first step of creating a structured plan.","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Digital-Gardens":{"title":"Digital Gardens","content":"Reference:  \nTopics:  [Productivity](Topics/Productivity.md)  \n\n---\n\nLorem ipsum bla bla","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Getting-Things-Done":{"title":"Getting Things Done","content":"\n","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Map-of-contents":{"title":"Map of contents","content":"## Topics\n- [Software Engineering](Topics/Software%20Engineering.md)\n- [Productivity](Topics/Productivity.md)\n- [Mental Models](Topics/Mental%20Models.md)\n","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Molecules/Task-implementation-framework":{"title":"Task implementation framework","content":"Topics: [Productivity](Topics/Productivity.md)  \nReference: [Brain Dump](Brain%20Dump.md) | [Activation Energy](Activation%20Energy.md)    \n\n---\nPersonal productivity created from a combination of insights from various sources with some real life testing to find something that works best for me.\n\n### Method\n- Brain dump\n- Use brain dump to create check list\n- Optional: Time block tasks from check list\n- Review how well the framework worked for that specific project/task.\n\n### Rationale\n- Brain dump - Helps to get the ball rolling when I don't know where to start\n- Creating a check list/plan before starting helps keep me focused in implementation phase. Having to pause to think what comes next or make a decision on what to tackle next can be just enough to get me distracted.\n- Time blocking - prevents me from taking too long on individual tasks. This can happen when due to analysis paralysis or perfectionism.\n\n\n","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Obsidian-System":{"title":"Obsidian System","content":"\n-   **Core types:**\n    - atom: the base note type for an individual piece of knowledge derived from a source\n    - molecule: bservation/insight drawn from a single or multiple atoms\n    - topic: Category placeholder. Helps find related notes in obsidian\n    - author: the creator of a piece of content\n    - todo: something I need to fill in\n-   **Source types and associated template:**\n    - book -\u003e book\n    - article: e.g blog post, web article -\u003e infomedia\n    - post: social media / forum -\u003e infomedia\n    - academic: textbook, journal article -\u003e academic\n    - video -\u003e infomedia\n    - podcast -\u003e infomedia\n-   **Atom types:**\n    - tool: something that I can use to solve a problem\n    - framework: a way of thinking about the world\n    - school-of-thought: a historical school of thought\n    - person: someone of note\n    - event: a historical event\n    - heuristic: a \"common wisdom\" way of doing something\n- **Workflow:**\n    -  Find interesting/useful info.\n    - Create a source if the media has multiple useful ideas/concepts. Create an atom if only 1.\n    - Atomise each useful concept.\n    - Come back to atoms every now and then.\n    - Create a molecule if a new insight is made.\n","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Parkinsons-law":{"title":"Parkinson's law","content":"Topics: [Productivity](Topics/Productivity.md)    \nReference: https://en.wikipedia.org/wiki/Parkinson%27s_law  \n\n---\n\n\"Work expands so as to fill the time available for it's completion\"\n\nThink of it as a law of physics to work around instead of as 'human nature'. This forces you to push this law to its limit for your own benefit:\n- Micro deadlines - aka 'Time blocking'\n- Experiment with shorter deadlines\n","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Sources/Atomic-Habits":{"title":"Atomic Habits","content":"\nAuthor: [James Clear](Authors/James%20Clear.md)  \nTopics: [Productivity](Topics/Productivity.md)  \n\n---\n\n## Theme. 1\n\n- Key idea 1 \n- Key idea 2\n\n## Theme 2\n\n- Key idea 3\n- Key idea 4\n","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Sources/Branchless-Programming-in-C++":{"title":"Branchless Programming in C++","content":"Link:  [Branchless programming in C++ CppCon video](https://www.youtube.com/watch?v=g-WPhYREFjk)  \nAuthor:  [Fedor Pikus](Authors/Fedor%20Pikus.md)  \nTopics: [Software Engineering](Topics/Software%20Engineering.md)  \n\n---\n\n## Potential benefits\n\n**Common code example 1:**\n```cpp\nf(bool b, unsigned long x, unsigned long\u0026 s)\n{\n    if (b) s+= x;\n}\n```\n130M calls/sec -\u003e 400M calls/sec optimised  \nOptimised version:  \n```cpp\nf(bool b, unsigned long x, unsigned long\u0026 s)\n{\n    s+= b * x; // use boolean as int to multiply 'x'\n}\n```\n\n**Common code example 2:**\n```cpp\nif (x[i] || y[i])\n{\n    // Do something\n}\n```\n150M evaluations/sec -\u003e 570M evaluations/sec\n(Optimisation show in \u003ca href=\"#2b-bitwise-optimisation\"\u003e2.b. bitwise optimisation\u003c/a\u003e)  \n\n## Philosophy for performance\n\nIn order of priority:\n- Get desired result with ==least work==\n    - Use an _optimal_ algorithm\n- Do not do any ==unnecessary work==\n    - Efficiently use language\n- Use ==all available resources==, ==at the same time==, ==all the time==.\n    - Efficient hardware use.\n\n### CPU compute resources\n**Inefficient use of CPU:**\n```cpp\nunsigned long v1[N], v2[N];\nunsigned long a = 0;\nfor (size_t i = 0; i \u003c N; ++i)\n{\n    a += v1[i] * v2[i];\n}\n```\n\nWhy is it inefficient use of CPU?  \nBecause it does not actually load the CPU very much at all as we are only doing one multiplication per iteration. In fact, more calculations can be thrown in for free.  \n\nThe following code will run at the same speed on modern CPUs.  \n```cpp\nunsigned long v1[N], v2[N];\nunsigned long a1 = 0, a2 = 0;\nfor (size_t i = 0; i \u003c N; ++i)\n{\n    a1 += v1[i] * v2[i];\n    a2 += v1[i] + v2[i];\n    ...\n    // in fact you can insert even more operations for 'free'\n}\n```\n\nSounds great! Not quite. You can almost never do that due to ==data dependencies==. In order to do the next computation, you need the result from the first. Things get even worse if there are ==code dependencies.== There may be branches and conditions which means that the CPU now must wait until it knows which ==instructions== to execute as well.  \n\nHaving so much compute power would be useless however if we didn't have some workarounds!\n\n## Pipelining\n```cpp\na += (v1[i] + v2[i]) * (v1[i] - v2[i]);\n```\n\nIn this example you can do the addition and subtraction in the first cycle, then do the multiplication in the second cycle, whilst also doing the addition and subtraction for the next iteration. This creates two _streams_ of instructions that are interleaved that have no data dependency between them at a given cpu cycle.  \n\nThis results in multiple instruction streams that have\n- Dependencies within each stream\n- No data dependencies between streams\n\nThis is great and increases cpu utilisation. However, the next barrier is... **conditional code.**\n\n## Branches\nPipelining requires a continuous stream of instructions. Conditions/Branches mean that the CPU is unsure of the next instruction to place into the pipeline. So what can we do?\n\n**Branch prediction**. The CPU guesses which branch to take and continues pipelining. The ==performance is now based on accuracy of predictions==. However, the branch can be mispredicted and recovering from a ==branch misprediction== is very costly.  \n\nBut how costly is this branch misprediction?\n\n## Experiments\n\n### 1.a. always true -\u003e same branch taken\n\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void always_true(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n       v1[i] = rand();\n       v2[i] = rand();\n       c1[i] = rand() \u003e= 0; // always true\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (b1[i]) {\n                a1 += p1[i];\n            } else {\n                a2 *= p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(always_true)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n\n**Benchmark result:**\n```shell\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.26, 0.35, 0.45\n\n------------------------------------------------------------------------------\nBenchmark                    Time             CPU   Iterations UserCounters...\n------------------------------------------------------------------------------\nalways_true/4194304    1741613 ns      1734898 ns          430 items_per_second=2.41761G/s\n\n```\n\n### 1.b. random -\u003e 50/50 if branch vs else branch\n\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void random(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n       v1[i] = rand();\n       v2[i] = rand();\n       c1[i] = rand() \u0026 0x1; // randomly true/false\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (b1[i]) {\n                a1 += p1[i];\n            } else {\n                a2 *= p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(random)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n\n**Benchmark result:**\n```shell\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.26, 0.36, 0.44\n\n-------------------------------------------------------------------------\nBenchmark               Time             CPU   Iterations UserCounters...\n-------------------------------------------------------------------------\nrandom/4194304   12131503 ns     12091002 ns           58 items_per_second=346.895M/s\n```\n\nAs you can see, the mispredicted benchmark is significantly slower. By about 7.7 times. Branch mispredictions are extremely costly.\n\nBut how do we detect when branch misprediction is a problem? By using a cpu profiling tool such as **==perf==**.\n\n### Using perf to detect branch mispredictions.\n\nThe command to run is `perf stat \u003cprogram\u003e`  \nThe result for the 1a the 'always true' program is:  \n```shell\ntask-clock:u                     #    1.000 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   58.862 K/sec\ncycles:u                         #    4.217 GHz                      (83.30%)\nstalled-cycles-frontend:u        #    1.63% frontend cycles idle     (83.35%)\nstalled-cycles-backend:u         #    0.17% backend cycles idle      (83.35%)\ninstructions:u                   #    3.68  insn per cycle\n                          #    0.00  stalled cycles per insn  (83.34%)\nbranches:u                       #    4.399 G/sec                    (83.35%)\nbranch-misses:u                  #    0.00% of all branches          (83.31%)\n```\n\n0% (or usually close to that) branch-misses. Compare that to 1b.\n\n```shell\ntask-clock:u                     #    1.000 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   52.165 K/sec\ncycles:u                         #    4.258 GHz                      (83.30%)\nstalled-cycles-frontend:u        #    0.87% frontend cycles idle     (83.31%)\nstalled-cycles-backend:u         #    0.00% backend cycles idle      (83.31%)\ninstructions:u                   #    0.88  insn per cycle\n                          #    0.01  stalled cycles per insn  (83.33%)\nbranches:u                       #    1.078 G/sec                    (83.40%)\nbranch-misses:u                  #   12.16% of all branches          (83.35%)\n```\n\n12% branch-misprediction. With the context of the second perf result, you can see that ==instructions per cycle== is also massively down from 3.68 -\u003e 0.88.  \n\n### 1.c. alternating if branch and else branch -\u003e predictable branching\n\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void alternating(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n        v1[i] = rand();\n        v2[i] = rand();\n        if (i == 0) c1[i] = rand() \u003e=0;\n        else c1[i] = !c1[i-1]; // alternate true and false\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (b1[i]) {\n                a1 += p1[i];\n            } else {\n                a2 *= p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(alternating)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n**Benchmark result:**\n```shell\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.89, 0.46, 0.42\n------------------------------------------------------------------------------\nBenchmark                    Time             CPU   Iterations UserCounters...\n------------------------------------------------------------------------------\nalternating/4194304    2529638 ns      2523583 ns          280 items_per_second=1.66204G/s\n```\n\n**Perf result:**\n```shell\ntask-clock:u                     #    0.999 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   57.934 K/sec\ncycles:u                         #    4.229 GHz                      (83.30%)\nstalled-cycles-frontend:u        #    0.74% frontend cycles idle     (83.32%)\nstalled-cycles-backend:u         #    0.22% backend cycles idle      (83.31%)\ninstructions:u                   #    2.39  insn per cycle\n                          #    0.00  stalled cycles per insn  (83.34%)\nbranches:u                       #    3.029 G/sec                    (83.39%)\nbranch-misses:u                  #    0.00% of all branches          (83.34%)\n```\n\nWith an alternating pattern, you can see that it is slower than the 'alway true' program, but faster than the horribly mispredicted one. 440 vs 58 vs 280.  \n\nThe branch-misses are also at 0%. The CPU figured out the pattern!  \n\n### 2.a. Predictable result, unpredictable branch\n\nThis is a case where you have an || condition.\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void random_predictable(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    std::vector\u003cint\u003e c2(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n        v1[i] = rand();\n        v2[i] = rand();\n        c1[i] = rand() \u0026 0x1;\n        c2[i] = !c1[i];\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    int* b2 = c2.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (b1[i] || b2[i]) {\n                a1 += p1[i];\n            } else {\n                a2 *= p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(random_predictable)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n\nthe 'if' condtion is always true as b1 is true when b2 isn't and viceversa. Although the result is predictable, the branch taken is not. This is because the branch is either through b1 or b2. Here are the benchmark results  \n\n```shell\nRunning ./02-random_predictable\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.42, 0.54, 0.54\n-------------------------------------------------------------------------------------\nBenchmark                           Time             CPU   Iterations UserCounters...\n-------------------------------------------------------------------------------------\nrandom_predictable/4194304   12137243 ns     12055659 ns           58 items_per_second=347.912M/s\n\ntask-clock:u                     #    1.000 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   65.941 K/sec\ncycles:u                         #    4.206 GHz                      (83.30%)\nstalled-cycles-frontend:u        #    0.85% frontend cycles idle     (83.30%)\nstalled-cycles-backend:u         #    0.45% backend cycles idle      (83.30%)\ninstructions:u                   #    1.01  insn per cycle\n                          #    0.01  stalled cycles per insn  (83.36%)\nbranches:u                       #    1.195 G/sec                    (83.39%)\nbranch-misses:u                  #   10.82% of all branches          (83.35%)\n```\n\nAs you can see the iterations value is down to 58 and branch-misses is up to 10.82%. Even though the result is the same every time.  \n\nHow can we optimise this?\n\n### 2.b. bitwise optimisation\nHere I will use addition. You can also use logical 'or'.\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void bitwise(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    std::vector\u003cint\u003e c2(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n        v1[i] = rand();\n        v2[i] = rand();\n        c1[i] = rand() \u0026 0x1;\n        c2[i] = !c1[i];\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    int* b2 = c2.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (bool(b1[i]) + bool(b2[i])) {\n                a1 += p1[i];\n            } else {\n                a2 *= p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(bitwise)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n```\n\nThe results:  \n```shell\nRunning ./02-bitwise\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.69, 0.50, 0.51\n\n--------------------------------------------------------------------------\nBenchmark                Time             CPU   Iterations UserCounters...\n--------------------------------------------------------------------------\nbitwise/4194304    2113774 ns      2097247 ns          337 items_per_second=1.99991G/s\n\n\ntask-clock:u                     #    1.000 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   73.555 K/sec\ncycles:u                         #    4.164 GHz                      (83.33%)\nstalled-cycles-frontend:u        #    1.04% frontend cycles idle     (83.33%)\nstalled-cycles-backend:u         #    0.36% backend cycles idle      (83.33%)\ninstructions:u                   #    4.60  insn per cycle\n                          #    0.00  stalled cycles per insn  (83.33%)\nbranches:u                       #    3.419 G/sec                    (83.33%)\nbranch-misses:u                  #    0.00% of all branches          (83.35%)\n```\n\nHere you can see branch-misses are down to 0% again and iterations are up from 58 to 337.\n\n**Bear in mind,** the optimisation is that instead of evaluating both conditions separately, you calculate both and then check both at the same time. This means you are doing more instructions. ==That is the tradeoff==. This can be important if you, for example, battery life is important to you.  \n\nAdditionally, if the result was not predictable, this would also instead be a performance hit.\n\n### 3.a. Branched unpredictable\nExperiment 3 will consist of taking a loop with 1 branch and showing how we can eliminate the branch completely. The first case is then the branched example.\n\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void branched_unpredictable(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n       v1[i] = rand();\n       v2[i] = rand();\n       c1[i] = rand() \u0026 0x1;\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (b1[i]) {\n                a1 += p1[i] - p2[i];\n            } else {\n                a2 *= p2[i] * p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(branched_unpredictable)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n\n**Result:**\n```shell\nRunning ./03-branched-unpredictable\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.76, 0.75, 0.63\n\n-----------------------------------------------------------------------------------------\nBenchmark                               Time             CPU   Iterations UserCounters...\n-----------------------------------------------------------------------------------------\nbranched_unpredictable/4194304   13162172 ns     13070350 ns           54 items_per_second=320.902M/s\n\ntask-clock:u                     #    0.999 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   51.062 K/sec\ncycles:u                         #    4.230 GHz                      (83.31%)\nstalled-cycles-frontend:u        #    0.87% frontend cycles idle     (83.32%)\nstalled-cycles-backend:u         #    0.20% backend cycles idle      (83.20%)\ninstructions:u                   #    0.88  insn per cycle\n                          #    0.01  stalled cycles per insn  (83.41%)\nbranches:u                       #    1.029 G/sec                    (83.40%)\nbranch-misses:u                  #   11.79% of all branches          (83.36%)\n```\n\n54 iterations, 11.79% branch-misses.\n\n### 3.b. branchless unpredictable\nThis example now shows how we can eliminate the branch completely.\n\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void branchless_unpredictable(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n       v1[i] = rand();\n       v2[i] = rand();\n       c1[i] = rand() \u0026 0x1;\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            unsigned long s1[2] = {0, p1[i] - p2[i]};\n            unsigned long s2[2] = {p1[i] * p2[i], 0};\n            a1 += s1[bool(b1[i])];\n            a2 += s2[bool(b1[i])];\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(branchless_unpredictable)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n\nWhat we are doing is essentially calculating both branches and storing them in an array. Then using the bool 'b1' as an integer to add both branch results to 'a1' and 'a2'. The key being that s1 and s2 store a '0' value in position 0 and 1 respectively. So how effective is this?\n\n```shell\nRunning ./03-branchless-unpredictable.cpp\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.30, 0.40, 0.51\n\n-------------------------------------------------------------------------------------------\nBenchmark                                 Time             CPU   Iterations UserCounters...\n-------------------------------------------------------------------------------------------\nbranchless_unpredictable/4194304    4246080 ns      4207040 ns          166 items_per_second=996.973M/s\n\ntask-clock:u                     #    0.998 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   48.458 K/sec\ncycles:u                         #    4.253 GHz                      (83.31%)\nstalled-cycles-frontend:u        #    1.16% frontend cycles idle     (83.34%)\nstalled-cycles-backend:u         #    0.17% backend cycles idle      (83.37%)\ninstructions:u                   #    3.69  insn per cycle\n                          #    0.00  stalled cycles per insn  (83.36%)\nbranches:u                       #    1.301 G/sec                    (83.29%)\nbranch-misses:u                  #    0.00% of all branches          (83.32%)\n```\n\nThe iterations have increased from 54 to 166 and branch-misses are now 0% again.  \n\nNote that the iterations has not increased as drastically as we are actually doing a lot more work, but there is still a significant performance boost.\n\nThis optimisation is effective under two circumstances:\n1. Extra computations are small\n2. Branch is poorly predicted\n\n## Closing thoughts\n\n- Predicted branches are cheap\n- Mispredictions are **very** expensive\n- **ALWAYS** use a profiler to detect optimisation locations.\n- Don't fight the compiler as it can often do the optimisations for you.","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Sources/Effective-C++":{"title":"Effective C++","content":"\nAuthor: [Scott Meyers](Authors/Scott%20Meyers.md)    \nTopics: [Software Engineering](Topics/Software%20Engineering.md)  \n\n---\n\n## 1 - (Chapter 1)\n\n- What the note is about:\n    - First point: organising notes this way makes them easier to read in retrospect\n    - Second point: it is also well-suited for the Obisidian Mind Map extension\n\n## 2 - (Chapter 2)\n","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Sources/Leon-Hendrix-Journaled-for-1000-days.-What-I-learned.":{"title":"Leon Hendrix - Journaled for 1000 days. What I learned.","content":"\nAuthor: [Leon Hendrix](Authors/Leon%20Hendrix.md)    \nLink: https://www.youtube.com/watch?v=0UhZDFK2Pwc  \nTopics: [Journaling](Topics/Journaling.md)  \n\n---\n\n### 7 key points\n- Vison journaling  (visualisation) - motivation\n    - Inspired by Arnold Schwarzenegger - \"When your vision is powerful enough, everything else falls into place\"\n    - **Write down what your ideal life would look like**\n- 'Positive worry' - motivation\n    - What if ...(series of good things)... happens to me?\n    - **Write down a series of good things that could happen to you.**\n- Focus on the one most important thing - productivity\n    - Gary Keller - The One thing\n    - Steve Jobs - \"People think focus means saying yes to the thing you've got to focus on. But that's not what it means at all. It means saying no to the hundred other good ideas that there are.\"\n    - Steps:\n        - **Make a list of everything you COULD do**\n        - **Narrow your focus. \"What is the one thing I could do that would make everything else easier or even unnecessary?\"**\n- Think bigger. 10x exercise - productivity\n- The brain dump - counter anxiety\n    - **Write down all thoughts. No filters. Until everything you're worried about is written down.**\n    - **Reassess**\n- To counter procastrination - productivity\n    - Usually because you need more clarity. Three most common reasons are:\n        - Unclear effort\n        - Unsure about capability/competence\n        - Unclear on outcome\n    - Steps:\n        - **Ask why are you procrastinating**\n        - **Write down resisting thoughts.**\n        - **Dig deeper. Ask follow up questions**\n- Gratitude on steroids - Mood elevation\n","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Sources/Mental-Models-The-Best-Way-to-Make-Intelligent-Decisions~100-Models-Explained":{"title":"Mental Models - The Best Way to Make Intelligent Decisions(~100 Models Explained)","content":"\nAuthor: [Rhiannon Beaubien](Authors/Rhiannon%20Beaubien.md)  \nLink: [fs blog](https://fs.blog/mental-models/)  \nTopics: [Mental Models](Topics/Mental%20Models.md)    \n\n---\n\n## What are they?\n- **Representation** of how something works.\n- **Simplify** the complex with models into **understandable and organisable** chunks.\n\n- Shapes how we think and how we understand.\n- Shapes **connections and opportunities** we see.\n\n## Learning to think better\n- More models -\u003e bigger toolbox -\u003e more likely to have right models to grasp reality.\n- Decision making ability improves with variety of models.\n\n- Engineer thinks in systems, pyschologist in incentives, biologist in evolution etc.\n- Combining disciplines -\u003e analyse problem in 3D way.\n- Practical wisdom is not just isolated facts. They need to hang together on latticework of theory.\n\n### Core Mental Models\n- **The Map is Not the Territory**\n- **Circle of Competence**\n\t- Know what you understand. Know where your edge is over others.\n\t- Understanding circle of competence improves decision-making and outcomes.\n- First Principles Thinking\n- Thought Experiment\n- **Second-Order Thinking**\n\t- Thinking farther ahead than immeidate results.\n\t- Thinking holistically.\n- Probabilistic Thinking\n- Inversion\n- Occam's Razor\n- Hanlon's Razor","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Topics/Journaling":{"title":"Journaling","content":"","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Topics/Mental-Models":{"title":"Mental Models","content":"","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Topics/Productivity":{"title":"Productivity","content":"","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null},"/Topics/Software-Engineering":{"title":"Software Engineering","content":"","lastmodified":"2023-02-09T13:26:24.771771385Z","tags":null}}