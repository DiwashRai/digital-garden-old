{"/":{"title":"Digital Garden Home ðŸŒ±","content":"Welcome to my [digital garden](Digital%20Gardens.md) - I'm Diwash. I have been exploring personal knowledge management for a long time and this site is my experiment with doing so in a more public manner that aligns with the idea of [work with the garage door up](Work%20with%20the%20garage%20door%20up.md).  \n\nYou will mainly find notes about software engineering and productivity, however I am just getting started and plan on talking more about AI, PKM systems, mental models and many more.  \n\nThe notes are laid out in a [network](Networked%20Thinking.md) which may be confusing at first, but should allow for easy exploration. For now feel free to use the graph below to dive right in or the map of contents to choose a topic.\n\n[Map of contents](Map%20of%20contents.md)\n","lastmodified":"2023-02-11T21:50:30.077551449Z","tags":null},"/Activation-Energy":{"title":"Activation Energy","content":"Reference:  https://www.makeuseof.com/tag/7-mental-models-get-work-done/ | [Atomic Habits](Sources/Atomic%20Habits.md)    \nTopics: [Productivity](Topics/Productivity.md)  \n\n---\nThe more complex a task, the higher the activation energy required to start and sustain it.\n- To counter this, target the energy required to **start** a task.\n- Break down tasks into simpler and smaller steps.","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Anti-marketing":{"title":"Anti-marketing","content":"Reference:  [Anti Marketing - Andy Matuschak](https://notes.andymatuschak.org/About_these_notes?stackedNotes=z21cgR9K3UcQ5a7yPsj2RUim3oM2TzdBByZu\u0026stackedNotes=z4bK6LaSBRetDzuYkeCs3A8mJ8DufTbK4o6FS)\nTopics: [Mental Models](Topics/Mental%20Models.md)  \n\n---\n\nA model for presenting your work that forces you to engage people more by talking about all the challenges that happened and how you struggled and overcame those challenges. This goes against most content out there where a presenter may want to make themselves or their project look as good as possible but leads to less personal and less genuine presentation.","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Authors/Fedor-Pikus":{"title":"Fedor Pikus","content":"","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Authors/James-Clear":{"title":"James Clear","content":"","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Authors/Leon-Hendrix":{"title":"Leon Hendrix","content":"","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Authors/Rhiannon-Beaubien":{"title":"Rhiannon Beaubien","content":"","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Authors/Scott-Meyers":{"title":"Scott Meyers","content":"","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Brain-Dump":{"title":"Brain Dump","content":"\nTopics: [Productivity](Topics/Productivity.md)  \nReference:[Leon Hendrix - Journaled for 1000 days. What I learned.](Sources/Leon%20Hendrix%20-%20Journaled%20for%201000%20days.%20What%20I%20learned..md) | [Getting Things Done](Getting%20Things%20Done.md)  \n\n---\n\nA technique used to offload all of the things that are currently straining you mentally by writing everything you can think of onto a note.\n\nValuable for reducing anxiety and staying productive as the first step of creating a structured plan.","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Digital-Gardens":{"title":"Digital Gardens","content":"References:  [Maggie Appleton - Brief History \u0026 Ethos of Digital Gardens](https://maggieappleton.com/garden-history)  \n[Tom Critchlow - Building digital garden](https://tomcritchlow.com/2019/02/17/building-digital-garden/)  \nTopics:  [Productivity](Topics/Productivity.md)  \n\n---\n\n### Intro\nDigital gardens are an alternative model for thinking about personal sites and blogs that goes against how we have come to think about them. They also support the philosophy of [Work with the garage door up](Work%20with%20the%20garage%20door%20up.md) as you are publishing your notes and thoughts online for all to see.  \n\n### Continuous growth\nDigital gardens treat content not as something that is finalised and published, but something that is in an ever growing state. You are free to make additions to each page when you learn something new or make a new connection.  \n\n### Network of concepts and ideas\nThe content in digital gardens are not laid out in some sort of time based 'stream' but as an [interconnected cluster of nodes](Networked%20Thinking.md). This allows for relationships between different topics/notes to be noticed and linked which can allow for better insights to be gained.  \n\n### Exploration over performance\nDigital gardens have a less performative philosophy. Each page is treated less like polished articles intended to be consumed by an audience, but more like a free form wiki of thought's an ideas that the person may note as they encounter them. You could argue that the primary audience for a digital garden is in fact the 'gardener' themselves. This idea frees the 'gardener' to not have to worry about making every sentence, every page perfect, but to focus on what they're interested in and what would benefit themselves (and perhaps other readers).  \n\n### Playful and experimental\nDigital gardens have a more playful and experimental nature to them. You will find a wide plethora of layouts, templates and languages used to create them. The aim is to find what works for you and to also have some fun so that you continue to maintain the garden. At the end of the day, the battle is to absorb the information that we are bombarded with daily in a way that can be applied in our own lives. For that to happen, we need to find what works for us and we have to stick with it.  \n\n### Summary\n- Digital gardens are continuously evolving and growing. No singular page or note should be considered truly complete as new connections and insights can always be made.\n- Digital gardens are a network of nodes. This allows ideas/concepts to be linked easier and for the value of knowledge growth to be non-linear.\n- Digital gardens value the creators own personal exploration and growth over presenting something as a finished product to an audience which brings with it a more performative mindset.\n- Digital gardens are playful and experimental as they provide more value to us, the more fun we are having and the more personalised the garden is to our own needs.","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Getting-Things-Done":{"title":"Getting Things Done","content":"\n","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Map-of-contents":{"title":"Map of contents","content":"## Topics\n- [Software Engineering](Topics/Software%20Engineering.md)\n- [Productivity](Topics/Productivity.md)\n- [Mental Models](Topics/Mental%20Models.md)\n","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Molecules/Task-implementation-framework":{"title":"Task implementation framework","content":"Topics: [Productivity](Topics/Productivity.md)  \nReference: [Brain Dump](Brain%20Dump.md) | [Activation Energy](Activation%20Energy.md)    \n\n---\nPersonal productivity created from a combination of insights from various sources with some real life testing to find something that works best for me.\n\n### Method\n- Brain dump\n- Use brain dump to create check list\n- Optional: Time block tasks from check list\n- Review how well the framework worked for that specific project/task.\n\n### Rationale\n- Brain dump - Helps to get the ball rolling when I don't know where to start\n- Creating a check list/plan before starting helps keep me focused in implementation phase. Having to pause to think what comes next or make a decision on what to tackle next can be just enough to get me distracted.\n- Time blocking - prevents me from taking too long on individual tasks. This can happen when due to analysis paralysis or perfectionism.\n\n\n","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Networked-Thinking":{"title":"Networked thinking","content":"Reference:  [Roam Research White Paper](Sources/Roam%20Research%20White%20Paper.md)\nTopics:  [Productivity](Topics/Productivity.md)\n\n---\n\nA new method of managing knowledge that is more suited to the modern information age where we have an unprecedented amount of information available. The idea is to think of each bit of knowledge as a singular node in an interconnected network of nodes instead of a more 'cabinet desk' like hierarchical strucuture for your knowledge.  \n\nBenefits include:\n- Ease of storage - No need to try to categorise a new bit of knowledge perfectly so that it sits in the correct location in your hierarchy.\n- Better recollection - You can remember an idea itself, or any other node that it may be connected to or even a series of links to the idea you want to recall.\n- Effortless cross-referencing - ideas can be cross-referenced easily without duplication.\n- Optimised for serendipity - graphical display of node network can allow unexpected insights and connections to be made.\n- 'Collaborative problem solving' - can help separate signal from the noise as you can consider conflicting opinions on the same topic and develop your own understanding or draw your own conclusions.","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Obsidian-System":{"title":"Obsidian System","content":"\n-   **Core types:**\n    - atom: the base note type for an individual piece of knowledge derived from a source\n    - molecule: bservation/insight drawn from a single or multiple atoms\n    - topic: Category placeholder. Helps find related notes in obsidian\n    - author: the creator of a piece of content\n    - todo: something I need to fill in\n-   **Source types and associated template:**\n    - book -\u003e book\n    - article: e.g blog post, web article -\u003e infomedia\n    - post: social media / forum -\u003e infomedia\n    - academic: textbook, journal article -\u003e academic\n    - video -\u003e infomedia\n    - podcast -\u003e infomedia\n-   **Atom types:**\n    - tool: something that I can use to solve a problem\n    - framework: a way of thinking about the world\n    - school-of-thought: a historical school of thought\n    - person: someone of note\n    - event: a historical event\n    - heuristic: a \"common wisdom\" way of doing something\n- **Workflow:**\n    -  Find interesting/useful info.\n    - Create a source if the media has multiple useful ideas/concepts. Create an atom if only 1.\n    - Atomise each useful concept.\n    - Come back to atoms every now and then.\n    - Create a molecule if a new insight is made.\n","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Parkinsons-law":{"title":"Parkinson's law","content":"Topics: [Productivity](Topics/Productivity.md)    \nReference: https://en.wikipedia.org/wiki/Parkinson%27s_law  \n\n---\n\n\"Work expands so as to fill the time available for it's completion\"\n\nThink of it as a law of physics to work around instead of as 'human nature'. This forces you to push this law to its limit for your own benefit:\n- Micro deadlines - aka 'Time blocking'\n- Experiment with shorter deadlines\n","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Sources/Atomic-Habits":{"title":"Atomic Habits","content":"\nAuthor: [James Clear](Authors/James%20Clear.md)  \nTopics: [Productivity](Topics/Productivity.md)  \n\n---\n\n## Theme. 1\n\n- Key idea 1 \n- Key idea 2\n\n## Theme 2\n\n- Key idea 3\n- Key idea 4\n","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Sources/Branchless-Programming-in-C++":{"title":"Branchless Programming in C++","content":"Link:  [Branchless programming in C++ CppCon video](https://www.youtube.com/watch?v=g-WPhYREFjk)  \nAuthor:  [Fedor Pikus](Authors/Fedor%20Pikus.md)  \nTopics: [Software Engineering](Topics/Software%20Engineering.md)  \n\n---\n\n## Potential benefits\n\n**Common code example 1:**\n```cpp\nf(bool b, unsigned long x, unsigned long\u0026 s)\n{\n    if (b) s+= x;\n}\n```\n130M calls/sec -\u003e 400M calls/sec optimised  \nOptimised version:  \n```cpp\nf(bool b, unsigned long x, unsigned long\u0026 s)\n{\n    s+= b * x; // use boolean as int to multiply 'x'\n}\n```\n\n**Common code example 2:**\n```cpp\nif (x[i] || y[i])\n{\n    // Do something\n}\n```\n150M evaluations/sec -\u003e 570M evaluations/sec\n(Optimisation show in \u003ca href=\"#2b-bitwise-optimisation\"\u003e2.b. bitwise optimisation\u003c/a\u003e)  \n\n## Philosophy for performance\n\nIn order of priority:\n- Get desired result with ==least work==\n    - Use an _optimal_ algorithm\n- Do not do any ==unnecessary work==\n    - Efficiently use language\n- Use ==all available resources==, ==at the same time==, ==all the time==.\n    - Efficient hardware use.\n\n### CPU compute resources\n**Inefficient use of CPU:**\n```cpp\nunsigned long v1[N], v2[N];\nunsigned long a = 0;\nfor (size_t i = 0; i \u003c N; ++i)\n{\n    a += v1[i] * v2[i];\n}\n```\n\nWhy is it inefficient use of CPU?  \nBecause it does not actually load the CPU very much at all as we are only doing one multiplication per iteration. In fact, more calculations can be thrown in for free.  \n\nThe following code will run at the same speed on modern CPUs.  \n```cpp\nunsigned long v1[N], v2[N];\nunsigned long a1 = 0, a2 = 0;\nfor (size_t i = 0; i \u003c N; ++i)\n{\n    a1 += v1[i] * v2[i];\n    a2 += v1[i] + v2[i];\n    ...\n    // in fact you can insert even more operations for 'free'\n}\n```\n\nSounds great! Not quite. You can almost never do that due to ==data dependencies==. In order to do the next computation, you need the result from the first. Things get even worse if there are ==code dependencies.== There may be branches and conditions which means that the CPU now must wait until it knows which ==instructions== to execute as well.  \n\nHaving so much compute power would be useless however if we didn't have some workarounds!\n\n## Pipelining\n```cpp\na += (v1[i] + v2[i]) * (v1[i] - v2[i]);\n```\n\nIn this example you can do the addition and subtraction in the first cycle, then do the multiplication in the second cycle, whilst also doing the addition and subtraction for the next iteration. This creates two _streams_ of instructions that are interleaved that have no data dependency between them at a given cpu cycle.  \n\nThis results in multiple instruction streams that have\n- Dependencies within each stream\n- No data dependencies between streams\n\nThis is great and increases cpu utilisation. However, the next barrier is... **conditional code.**\n\n## Branches\nPipelining requires a continuous stream of instructions. Conditions/Branches mean that the CPU is unsure of the next instruction to place into the pipeline. So what can we do?\n\n**Branch prediction**. The CPU guesses which branch to take and continues pipelining. The ==performance is now based on accuracy of predictions==. However, the branch can be mispredicted and recovering from a ==branch misprediction== is very costly.  \n\nBut how costly is this branch misprediction?\n\n## Experiments\n\n### 1.a. always true -\u003e same branch taken\n\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void always_true(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n       v1[i] = rand();\n       v2[i] = rand();\n       c1[i] = rand() \u003e= 0; // always true\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (b1[i]) {\n                a1 += p1[i];\n            } else {\n                a2 *= p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(always_true)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n\n**Benchmark result:**\n```shell\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.26, 0.35, 0.45\n\n------------------------------------------------------------------------------\nBenchmark                    Time             CPU   Iterations UserCounters...\n------------------------------------------------------------------------------\nalways_true/4194304    1741613 ns      1734898 ns          430 items_per_second=2.41761G/s\n\n```\n\n### 1.b. random -\u003e 50/50 if branch vs else branch\n\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void random(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n       v1[i] = rand();\n       v2[i] = rand();\n       c1[i] = rand() \u0026 0x1; // randomly true/false\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (b1[i]) {\n                a1 += p1[i];\n            } else {\n                a2 *= p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(random)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n\n**Benchmark result:**\n```shell\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.26, 0.36, 0.44\n\n-------------------------------------------------------------------------\nBenchmark               Time             CPU   Iterations UserCounters...\n-------------------------------------------------------------------------\nrandom/4194304   12131503 ns     12091002 ns           58 items_per_second=346.895M/s\n```\n\nAs you can see, the mispredicted benchmark is significantly slower. By about 7.7 times. Branch mispredictions are extremely costly.\n\nBut how do we detect when branch misprediction is a problem? By using a cpu profiling tool such as **==perf==**.\n\n### Using perf to detect branch mispredictions.\n\nThe command to run is `perf stat \u003cprogram\u003e`  \nThe result for the 1a the 'always true' program is:  \n```shell\ntask-clock:u                     #    1.000 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   58.862 K/sec\ncycles:u                         #    4.217 GHz                      (83.30%)\nstalled-cycles-frontend:u        #    1.63% frontend cycles idle     (83.35%)\nstalled-cycles-backend:u         #    0.17% backend cycles idle      (83.35%)\ninstructions:u                   #    3.68  insn per cycle\n                          #    0.00  stalled cycles per insn  (83.34%)\nbranches:u                       #    4.399 G/sec                    (83.35%)\nbranch-misses:u                  #    0.00% of all branches          (83.31%)\n```\n\n0% (or usually close to that) branch-misses. Compare that to 1b.\n\n```shell\ntask-clock:u                     #    1.000 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   52.165 K/sec\ncycles:u                         #    4.258 GHz                      (83.30%)\nstalled-cycles-frontend:u        #    0.87% frontend cycles idle     (83.31%)\nstalled-cycles-backend:u         #    0.00% backend cycles idle      (83.31%)\ninstructions:u                   #    0.88  insn per cycle\n                          #    0.01  stalled cycles per insn  (83.33%)\nbranches:u                       #    1.078 G/sec                    (83.40%)\nbranch-misses:u                  #   12.16% of all branches          (83.35%)\n```\n\n12% branch-misprediction. With the context of the second perf result, you can see that ==instructions per cycle== is also massively down from 3.68 -\u003e 0.88.  \n\n### 1.c. alternating if branch and else branch -\u003e predictable branching\n\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void alternating(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n        v1[i] = rand();\n        v2[i] = rand();\n        if (i == 0) c1[i] = rand() \u003e=0;\n        else c1[i] = !c1[i-1]; // alternate true and false\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (b1[i]) {\n                a1 += p1[i];\n            } else {\n                a2 *= p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(alternating)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n**Benchmark result:**\n```shell\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.89, 0.46, 0.42\n------------------------------------------------------------------------------\nBenchmark                    Time             CPU   Iterations UserCounters...\n------------------------------------------------------------------------------\nalternating/4194304    2529638 ns      2523583 ns          280 items_per_second=1.66204G/s\n```\n\n**Perf result:**\n```shell\ntask-clock:u                     #    0.999 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   57.934 K/sec\ncycles:u                         #    4.229 GHz                      (83.30%)\nstalled-cycles-frontend:u        #    0.74% frontend cycles idle     (83.32%)\nstalled-cycles-backend:u         #    0.22% backend cycles idle      (83.31%)\ninstructions:u                   #    2.39  insn per cycle\n                          #    0.00  stalled cycles per insn  (83.34%)\nbranches:u                       #    3.029 G/sec                    (83.39%)\nbranch-misses:u                  #    0.00% of all branches          (83.34%)\n```\n\nWith an alternating pattern, you can see that it is slower than the 'alway true' program, but faster than the horribly mispredicted one. 440 vs 58 vs 280.  \n\nThe branch-misses are also at 0%. The CPU figured out the pattern!  \n\n### 2.a. Predictable result, unpredictable branch\n\nThis is a case where you have an || condition.\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void random_predictable(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    std::vector\u003cint\u003e c2(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n        v1[i] = rand();\n        v2[i] = rand();\n        c1[i] = rand() \u0026 0x1;\n        c2[i] = !c1[i];\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    int* b2 = c2.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (b1[i] || b2[i]) {\n                a1 += p1[i];\n            } else {\n                a2 *= p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(random_predictable)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n\nthe 'if' condtion is always true as b1 is true when b2 isn't and viceversa. Although the result is predictable, the branch taken is not. This is because the branch is either through b1 or b2. Here are the benchmark results  \n\n```shell\nRunning ./02-random_predictable\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.42, 0.54, 0.54\n-------------------------------------------------------------------------------------\nBenchmark                           Time             CPU   Iterations UserCounters...\n-------------------------------------------------------------------------------------\nrandom_predictable/4194304   12137243 ns     12055659 ns           58 items_per_second=347.912M/s\n\ntask-clock:u                     #    1.000 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   65.941 K/sec\ncycles:u                         #    4.206 GHz                      (83.30%)\nstalled-cycles-frontend:u        #    0.85% frontend cycles idle     (83.30%)\nstalled-cycles-backend:u         #    0.45% backend cycles idle      (83.30%)\ninstructions:u                   #    1.01  insn per cycle\n                          #    0.01  stalled cycles per insn  (83.36%)\nbranches:u                       #    1.195 G/sec                    (83.39%)\nbranch-misses:u                  #   10.82% of all branches          (83.35%)\n```\n\nAs you can see the iterations value is down to 58 and branch-misses is up to 10.82%. Even though the result is the same every time.  \n\nHow can we optimise this?\n\n### 2.b. bitwise optimisation\nHere I will use addition. You can also use logical 'or'.\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void bitwise(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    std::vector\u003cint\u003e c2(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n        v1[i] = rand();\n        v2[i] = rand();\n        c1[i] = rand() \u0026 0x1;\n        c2[i] = !c1[i];\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    int* b2 = c2.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (bool(b1[i]) + bool(b2[i])) {\n                a1 += p1[i];\n            } else {\n                a2 *= p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(bitwise)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n```\n\nThe results:  \n```shell\nRunning ./02-bitwise\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.69, 0.50, 0.51\n\n--------------------------------------------------------------------------\nBenchmark                Time             CPU   Iterations UserCounters...\n--------------------------------------------------------------------------\nbitwise/4194304    2113774 ns      2097247 ns          337 items_per_second=1.99991G/s\n\n\ntask-clock:u                     #    1.000 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   73.555 K/sec\ncycles:u                         #    4.164 GHz                      (83.33%)\nstalled-cycles-frontend:u        #    1.04% frontend cycles idle     (83.33%)\nstalled-cycles-backend:u         #    0.36% backend cycles idle      (83.33%)\ninstructions:u                   #    4.60  insn per cycle\n                          #    0.00  stalled cycles per insn  (83.33%)\nbranches:u                       #    3.419 G/sec                    (83.33%)\nbranch-misses:u                  #    0.00% of all branches          (83.35%)\n```\n\nHere you can see branch-misses are down to 0% again and iterations are up from 58 to 337.\n\n**Bear in mind,** the optimisation is that instead of evaluating both conditions separately, you calculate both and then check both at the same time. This means you are doing more instructions. ==That is the tradeoff==. This can be important if you, for example, battery life is important to you.  \n\nAdditionally, if the result was not predictable, this would also instead be a performance hit.\n\n### 3.a. Branched unpredictable\nExperiment 3 will consist of taking a loop with 1 branch and showing how we can eliminate the branch completely. The first case is then the branched example.\n\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void branched_unpredictable(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n       v1[i] = rand();\n       v2[i] = rand();\n       c1[i] = rand() \u0026 0x1;\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            if (b1[i]) {\n                a1 += p1[i] - p2[i];\n            } else {\n                a2 *= p2[i] * p2[i];\n            }\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(branched_unpredictable)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n\n**Result:**\n```shell\nRunning ./03-branched-unpredictable\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.76, 0.75, 0.63\n\n-----------------------------------------------------------------------------------------\nBenchmark                               Time             CPU   Iterations UserCounters...\n-----------------------------------------------------------------------------------------\nbranched_unpredictable/4194304   13162172 ns     13070350 ns           54 items_per_second=320.902M/s\n\ntask-clock:u                     #    0.999 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   51.062 K/sec\ncycles:u                         #    4.230 GHz                      (83.31%)\nstalled-cycles-frontend:u        #    0.87% frontend cycles idle     (83.32%)\nstalled-cycles-backend:u         #    0.20% backend cycles idle      (83.20%)\ninstructions:u                   #    0.88  insn per cycle\n                          #    0.01  stalled cycles per insn  (83.41%)\nbranches:u                       #    1.029 G/sec                    (83.40%)\nbranch-misses:u                  #   11.79% of all branches          (83.36%)\n```\n\n54 iterations, 11.79% branch-misses.\n\n### 3.b. branchless unpredictable\nThis example now shows how we can eliminate the branch completely.\n\n```cpp\n\n#include \"benchmark/benchmark.h\"\n\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\nstatic void branchless_unpredictable(benchmark::State\u0026 state) {\n    srand(1);\n    const unsigned int N = state.range(0);\n    std::vector\u003cunsigned long\u003e v1(N), v2(N);\n    std::vector\u003cint\u003e c1(N);\n    for (size_t i = 0; i \u003c N; ++i) {\n       v1[i] = rand();\n       v2[i] = rand();\n       c1[i] = rand() \u0026 0x1;\n    }\n    unsigned long* p1 = v1.data();\n    unsigned long* p2 = v2.data();\n    int* b1 = c1.data();\n    for (auto _ : state) {\n        unsigned long a1 = 0, a2 = 0;\n        for (size_t i = 0; i \u003c N; ++i) {\n            unsigned long s1[2] = {0, p1[i] - p2[i]};\n            unsigned long s2[2] = {p1[i] * p2[i], 0};\n            a1 += s1[bool(b1[i])];\n            a2 += s2[bool(b1[i])];\n        }\n        benchmark::DoNotOptimize(a1);\n        benchmark::DoNotOptimize(a2);\n        benchmark::ClobberMemory();\n    }\n    state.SetItemsProcessed(N*state.iterations());\n}\n\nBENCHMARK(branchless_unpredictable)-\u003eArg(1\u003c\u003c22);\n\nBENCHMARK_MAIN();\n\n```\n\nWhat we are doing is essentially calculating both branches and storing them in an array. Then using the bool 'b1' as an integer to add both branch results to 'a1' and 'a2'. The key being that s1 and s2 store a '0' value in position 0 and 1 respectively. So how effective is this?\n\n```shell\nRunning ./03-branchless-unpredictable.cpp\nRun on (12 X 4467.28 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x6)\n  L1 Instruction 32 KiB (x6)\n  L2 Unified 512 KiB (x6)\n  L3 Unified 32768 KiB (x1)\nLoad Average: 0.30, 0.40, 0.51\n\n-------------------------------------------------------------------------------------------\nBenchmark                                 Time             CPU   Iterations UserCounters...\n-------------------------------------------------------------------------------------------\nbranchless_unpredictable/4194304    4246080 ns      4207040 ns          166 items_per_second=996.973M/s\n\ntask-clock:u                     #    0.998 CPUs utilized\ncontext-switches:u               #    0.000 /sec\ncpu-migrations:u                 #    0.000 /sec\npage-faults:u                    #   48.458 K/sec\ncycles:u                         #    4.253 GHz                      (83.31%)\nstalled-cycles-frontend:u        #    1.16% frontend cycles idle     (83.34%)\nstalled-cycles-backend:u         #    0.17% backend cycles idle      (83.37%)\ninstructions:u                   #    3.69  insn per cycle\n                          #    0.00  stalled cycles per insn  (83.36%)\nbranches:u                       #    1.301 G/sec                    (83.29%)\nbranch-misses:u                  #    0.00% of all branches          (83.32%)\n```\n\nThe iterations have increased from 54 to 166 and branch-misses are now 0% again.  \n\nNote that the iterations has not increased as drastically as we are actually doing a lot more work, but there is still a significant performance boost.\n\nThis optimisation is effective under two circumstances:\n1. Extra computations are small\n2. Branch is poorly predicted\n\n## Closing thoughts\n\n- Predicted branches are cheap\n- Mispredictions are **very** expensive\n- **ALWAYS** use a profiler to detect optimisation locations.\n- Don't fight the compiler as it can often do the optimisations for you.","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Sources/Effective-C++":{"title":"Effective C++","content":"\nAuthor: [Scott Meyers](Authors/Scott%20Meyers.md)    \nTopics: [Software Engineering](Topics/Software%20Engineering.md)  \n\n---\n\n## 1 - (Chapter 1)\n\n- What the note is about:\n    - First point: organising notes this way makes them easier to read in retrospect\n    - Second point: it is also well-suited for the Obisidian Mind Map extension\n\n## 2 - (Chapter 2)\n","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Sources/Leon-Hendrix-Journaled-for-1000-days.-What-I-learned.":{"title":"Leon Hendrix - Journaled for 1000 days. What I learned.","content":"\nAuthor: [Leon Hendrix](Authors/Leon%20Hendrix.md)    \nLink: https://www.youtube.com/watch?v=0UhZDFK2Pwc  \nTopics: [Journaling](Topics/Journaling.md)  \n\n---\n\n### 7 key points\n- Vison journaling  (visualisation) - motivation\n    - Inspired by Arnold Schwarzenegger - \"When your vision is powerful enough, everything else falls into place\"\n    - **Write down what your ideal life would look like**\n- 'Positive worry' - motivation\n    - What if ...(series of good things)... happens to me?\n    - **Write down a series of good things that could happen to you.**\n- Focus on the one most important thing - productivity\n    - Gary Keller - The One thing\n    - Steve Jobs - \"People think focus means saying yes to the thing you've got to focus on. But that's not what it means at all. It means saying no to the hundred other good ideas that there are.\"\n    - Steps:\n        - **Make a list of everything you COULD do**\n        - **Narrow your focus. \"What is the one thing I could do that would make everything else easier or even unnecessary?\"**\n- Think bigger. 10x exercise - productivity\n- The brain dump - counter anxiety\n    - **Write down all thoughts. No filters. Until everything you're worried about is written down.**\n    - **Reassess**\n- To counter procastrination - productivity\n    - Usually because you need more clarity. Three most common reasons are:\n        - Unclear effort\n        - Unsure about capability/competence\n        - Unclear on outcome\n    - Steps:\n        - **Ask why are you procrastinating**\n        - **Write down resisting thoughts.**\n        - **Dig deeper. Ask follow up questions**\n- Gratitude on steroids - Mood elevation\n","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Sources/Mental-Models-The-Best-Way-to-Make-Intelligent-Decisions~100-Models-Explained":{"title":"Mental Models - The Best Way to Make Intelligent Decisions(~100 Models Explained)","content":"\nAuthor: [Rhiannon Beaubien](Authors/Rhiannon%20Beaubien.md)  \nLink: [fs blog](https://fs.blog/mental-models/)  \nTopics: [Mental Models](Topics/Mental%20Models.md)    \n\n---\n\n## What are they?\n- **Representation** of how something works.\n- **Simplify** the complex with models into **understandable and organisable** chunks.\n\n- Shapes how we think and how we understand.\n- Shapes **connections and opportunities** we see.\n\n## Learning to think better\n- More models -\u003e bigger toolbox -\u003e more likely to have right models to grasp reality.\n- Decision making ability improves with variety of models.\n\n- Engineer thinks in systems, pyschologist in incentives, biologist in evolution etc.\n- Combining disciplines -\u003e analyse problem in 3D way.\n- Practical wisdom is not just isolated facts. They need to hang together on latticework of theory.\n\n### Core Mental Models\n- **The Map is Not the Territory**\n- **Circle of Competence**\n\t- Know what you understand. Know where your edge is over others.\n\t- Understanding circle of competence improves decision-making and outcomes.\n- First Principles Thinking\n- Thought Experiment\n- **Second-Order Thinking**\n\t- Thinking farther ahead than immeidate results.\n\t- Thinking holistically.\n- Probabilistic Thinking\n- Inversion\n- Occam's Razor\n- Hanlon's Razor","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Sources/Roam-Research-White-Paper":{"title":"Roam Research White Paper","content":"citation:  [White Paper](https://roamresearch.com/#/app/help/page/dZ72V0Ig6)  \nTopics:  [Productivity](Topics/Productivity.md)  \n\n---\nWe are experiencing an unprecedented explosion of knowledge, yet neither the human brain nor current technologies are equipped to ==harnesess== it to its ==full potential==.\n\n## Brief\n- Current approaches to knowledge management are ill suited for information age. They are usually 'cabinet like'.\n\t- Note: Presumable being hierarchical and isolated from one another.\n- Cabinet approach makes it difficult to share ideas across your files and lack interconnectivity.\n- Knowledge graphs are a more flexible data structure to tackle this challenge of knowledge management.\n- Basic users: ease of storage, recollection of ideas and cross-referencing of ideas\n- Power users: applications in logic and reasoning.\n- ==Optimising for serendipity==. Humans have difficulty generating random numbers, thoughts etc. Exposure to random 'noise' can be the stimuli to new insights. These new insights usually occur at the juncture of two seemingly unrelated concepts.\n- Roam's interconnectivity constantly creates opportunities for serendipity and new insights.\n- _Collaborative problem solving - separating the noise from the signal:_ Current learning/research protocols are bound by necessary consensus. As information increases, so does noise, making it difficult to come to a consensus. A curated knowledge graph allows weighing up of conflicting opinions and ideas to develop your own understanding without autocracy (listening to one trusted source) or democracy (waiting for a full consensus that might never come).\n","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Topics/Journaling":{"title":"Journaling","content":"","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Topics/Mental-Models":{"title":"Mental Models","content":"","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Topics/Productivity":{"title":"Productivity","content":"","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Topics/Software-Engineering":{"title":"Software Engineering","content":"","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null},"/Work-with-the-garage-door-up":{"title":"Work with the garage door up","content":"References:  [Work with the garage door up - Andy Matuschak](https://notes.andymatuschak.org/About_these_notes?stackedNotes=z21cgR9K3UcQ5a7yPsj2RUim3oM2TzdBByZu\u0026stackedNotes=z2DABWsGLkXcCuUet2scfD1duL1ZHBztwGKp) [Learn in public](https://www.swyx.io/learn-in-public)\nTopics:  [Productivity](Topics/Productivity.md)  \n\n---\n\n\"Work with the garage door up\" is the idea that you should share what you're working on and learning to the public. The idea is that by sharing your work, you can document your progress and allow people to provide feedback, inspiration and support.  \n\nWorking with the garage door up can also have a motivating aspect to it. If you are publishing your development, you are more likely to keep consistent as well as to think through your ideas more. The transparency and openness also makes it naturally align with the philosophy of [anti-marketing](Anti-marketing.md).     \n\nIt seems to have similarities with the 'rubber duck debugging' method where even if no one is reading your notes/posts, the idea that it might be read by other people forces your mind to process the information more carefully which makes your own learning more effective.","lastmodified":"2023-02-11T21:50:30.07355142Z","tags":null}}