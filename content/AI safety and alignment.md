---
title: ""
tags:
- MOC
---
Topics: [AI](Topics/AI.md)  

---

## Existential risk perspective

So here's the fundamental stack of arguments for why AI is an exisential
risk for humans:
1. Intelligence is an incredibly powerful force and orthogonal to goals and
values.
2. AI systems tend to converge on similar instrumental goals.
3. Crafting the perfect utility function for AI is exceptionally difficult.
4. Capturing human values in an AI system is an immense challenge and the
default assumption when we give an AI 'values' is that it does not match
human values.

Each point builds upon the other and is actually a pretty good way to dive
into AI safety. Here is a good sequence of topics to cover. You can
start with the corresponding youtube video before proceeding into papers.

### AI orthogonality thesis

[Intelligence and stupidity - Robert Miles](https://www.youtube.com/watch?v=hEUO6pjwFOo)  
Further reading: [The Superintelligent Will - Nick Bostrom](https://nickbostrom.com/superintelligentwill.pdf)  

### Instrumental convergence
[Why would AI want to do bad things? - Robert Miles](https://www.youtube.com/watch?v=ZeecOKBus3Q)  
Further reading: [The Nature of self-improving artificial intelligence - Stephen Omohundro](https://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf)  

### Challenges in aligning AI utility functions
Oh boy, here's the fun one...

To begin with lets see some concrete examples of how telling an AI what
you want can go wrong:
- [9 Examples of specification gaming - Robert Miles](https://www.youtube.com/watch?v=nKJlF-olKmg)
    - 6:08 for AI 'tricking' human into thinking it's going to pick up the ball
- [We were right! Real inner misalignment - Robert Miles](https://www.youtube.com/watch?v=zkbPdEHEyEI)
- [Deadly truth of general AI - Computerphile](https://www.youtube.com/watch?v=tcdVC4e6EV4)
    - Thought experiment that does something useful in demonstrating that
an existential threat does not look like something out of movies e.g.
Terminator

If you want to do dive even further, the next topics to look into would be
inner and outer alignment aka Mesa optimisation.

## Societal impacts perspective

Just to name a few:
- Mass misinformation
- Deepfakes
- Job loss
- AI for military applications
- Mass AI political campaigns
- Security
    - AI generated malware
    - AI to bypass authentication
    - AI use in social engineering e.g. AI to copy voice
- Lack of transparency
    - Difficult to hold individuals and organisations accountable due to
opaque nature of AI
